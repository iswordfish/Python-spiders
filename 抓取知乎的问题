# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf8')
# 爬取知乎一个主题下所有的问题
import urllib2
import re


class ZhiHu:
    # 初始化方法，定义变量
    def __init__(self):
        self.pageIndex = 1
        self.user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
        # 初始化 headers
        self.headers = {'User-Agent': self.user_agent}
        # 存放问题的列表，每个元素是一页问题（20 个）
        self.questions = []
        # 爬虫运行状态
        self.enable = False

    # 获取页面 HTML
    def getPage(self, pageIndex):
        try:
            if pageIndex != 1:
                url = 'https://www.zhihu.com/topic/19571876/questions?page=' + str(pageIndex)

            else:
                url ='https://www.zhihu.com/topic/19571876/questions'


            # 构建请求的 request
            request = urllib2.Request(url)
            # 获取请求的页面 HTML 代码
            response = urllib2.urlopen(request)
            # 将页面代码转成 UTF-8
            pageCode = response.read().decode('utf-8')
            #print(pageCode)
            return pageCode
        except urllib2.HTTPError, e:
            if hasattr(e, 'reason'):
                print u'连接知识错误，原因：', e.reason
                return None

    # 传入某一页页码，返回问题列表
    def getPageItem(self, pageIndex):
        pageCode = self.getPage(pageIndex)
        # 存放每页问题的列表，每个元素包括问题的 ID 及标题
        pageQuestions = []
        # 判断页面是否获取成功
        if not pageCode:
            print u'页面加载失败...'
            return None
        # 正则匹配每个问题 URL 中的 ID 及标题
        pattern = re.compile(r'href="/question/(.*?)">(.*?){}</a>', re.S)
        items = re.findall(pattern, pageCode)
        # 遍历正则表达匹配的结果，并存入到 pageQuestions
        for item in items:
            # item[0] 是问题的 ID，item[1]问题的标题
            pageQuestions.append([item[0], item[1]])
        return pageQuestions

    # 加载并提取页面的内容加入到列表中
    def loadPage(self):
        if self.enable == True:
            # 如果当前未看的页数少于 2 页，则加载下一页
            if len(self.questions) < 2:
                # 获取新的一页
                pageQuestions = self.getPageItem(self.pageIndex)
                if pageQuestions:
                    # 将该页问题存放到全局列表中
                    self.questions.append(pageQuestions)
                    # 获取完后，页面索引 +1
                    self.pageIndex += 1

    # 打印一页问题
    def getQuestion(self, pageQuestions, page):
        for question in pageQuestions:
            print u'第 %s 页，问题 %s：%s' % (page, question[0], question[1])

    # 开始方法
    def start(self):
        print u'正在读取知乎问题，按回车查看，Q 退出'
        self.enable = True
        nowPage = 0
        while self.enable:
            # 输入命令
            input = raw_input('please decise whether continue：')
            self.loadPage()
            # 输入「Q」则停止程序
            if input == 'Q':
                self.enable = False
                break
            elif len(self.questions) > 0:
                pageQuestions = self.questions[0]
                nowPage += 1
                del self.questions[0]
                self.getQuestion(pageQuestions, nowPage)
            print input


# 实例化知乎爬虫
spider = ZhiHu()
# 运行爬虫
spider.start()
